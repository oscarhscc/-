{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2id, id2tag = {}, {}\n",
    "word2id, id2word = {}, {}\n",
    "\n",
    "for line in open('dataset/pos_tag_dataset.txt', encoding='utf-8'):\n",
    "    if line:\n",
    "        for items in line.split('  '):\n",
    "            item = items.split('/')\n",
    "            if len(item)==2:\n",
    "                word, tag = item[0], item[1].rstrip()\n",
    "\n",
    "                if word not in word2id:\n",
    "                    word2id[word] = len(word2id)\n",
    "                    id2word[len(id2word)] = word\n",
    "                if tag not in tag2id:\n",
    "                    tag2id[tag] = len(tag2id)\n",
    "                    id2tag[len(id2tag)] = tag\n",
    "\n",
    "M = len(word2id)\n",
    "N = len(tag2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M:55317 N:45\n"
     ]
    }
   ],
   "source": [
    "print('M:{} N:{}'.format(M, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'v', 1: 'n', 2: 'u', 3: 'a', 4: 'w', 5: 't', 6: 'm', 7: 'q', 8: 'nt', 9: 'nr', 10: 'Vg', 11: 'k', 12: 'p', 13: 'f', 14: 'r', 15: 'vn', 16: 'ns', 17: 'c', 18: 's', 19: 'd', 20: 'ad', 21: 'j', 22: 'l', 23: 'an', 24: 'b', 25: 'i', 26: 'vd', 27: 'z', 28: 'nz', 29: 'Ng', 30: 'Tg', 31: 'y', 32: 'nx', 33: 'vnz', 34: 'Ag', 35: 'o', 36: 'Dg', 37: 'Bg', 38: 'h', 39: 'Rg', 40: 'e', 41: 'vnt', 42: 'Mg', 43: 'na', 44: 'Yg'}\n"
     ]
    }
   ],
   "source": [
    "print(id2tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建 pi A B\n",
    "import numpy as np\n",
    "pi = np.zeros(N)\n",
    "A = np.zeros((N, M))\n",
    "B = np.zeros((N, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in open('dataset/pos_tag_dataset.txt',encoding='utf-8'):\n",
    "    if line:\n",
    "        prev_tag = ''\n",
    "        for items in line.split('  '):\n",
    "            item = items.split('/')\n",
    "            if len(item)==2:\n",
    "                wordId, tagId = word2id[item[0]], tag2id[item[1].rstrip()]\n",
    "                if prev_tag == '': # 句子的开始\n",
    "                    pi[tagId] += 1\n",
    "                    A[tagId][wordId] += 1\n",
    "                else:\n",
    "                    A[tagId][wordId] += 1\n",
    "                    B[tag2id[prev_tag]][tagId] += 1\n",
    "\n",
    "                if item[0] == '。':\n",
    "                    prev_tag = ''\n",
    "                else:\n",
    "                    prev_tag = item[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.271e+03, 5.414e+03, 9.000e+00, 5.840e+02, 3.544e+03, 3.245e+03,\n",
       "       2.356e+03, 1.000e+01, 7.950e+02, 3.148e+03, 4.100e+01, 0.000e+00,\n",
       "       4.735e+03, 1.760e+02, 6.418e+03, 3.760e+02, 3.004e+03, 2.405e+03,\n",
       "       1.460e+02, 1.413e+03, 1.190e+02, 6.280e+02, 3.980e+02, 1.400e+01,\n",
       "       2.020e+02, 1.770e+02, 3.000e+00, 5.200e+01, 2.310e+02, 2.400e+01,\n",
       "       2.600e+01, 1.000e+00, 1.700e+01, 0.000e+00, 5.000e+00, 4.000e+00,\n",
       "       6.000e+00, 0.000e+00, 1.000e+00, 0.000e+00, 1.000e+01, 0.000e+00,\n",
       "       0.000e+00, 0.000e+00, 0.000e+00])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "pi = pi / sum(pi)\n",
    "for i in range(N):\n",
    "    A[i] /= sum(A[i])\n",
    "    B[i] /= sum(B[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(x):\n",
    "    if x == 0:\n",
    "        return np.log(0.00001)\n",
    "    else:\n",
    "        return np.log(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(x, pi, A, B):\n",
    "    '''\n",
    "    x: 输入的句子\n",
    "    pi:初始状态\n",
    "    A:发射概率\n",
    "    B:转移概率\n",
    "    '''\n",
    "    seg_list = jieba.cut(x)\n",
    "    x = [word2id[word] for word in seg_list]\n",
    "    T = len(x)\n",
    "    \n",
    "    dp = np.zeros((T,N)) # dp[i][j]:w1,w2,...,wT, 假设wi的tag是第j个tag\n",
    "    # basecase for dp algorithm\n",
    "    pointer = np.array([[0 for x in range(N)] for y in range(T)]) # T*N\n",
    "    \n",
    "    for j in range(N):\n",
    "        dp[0][j] = log(pi[j]) + log(A[j][x[0]])\n",
    "    for i in range(1, T): # 词语\n",
    "        for j in range(N): # 词性\n",
    "            dp[i][j] = float(\"-inf\")\n",
    "            for k in range(N): # 从每一个k词性到j\n",
    "                score = dp[i-1][k] + log(B[k][j]) + log(A[j][x[i]])\n",
    "                if score > dp[i][j]:\n",
    "                    dp[i][j] = score\n",
    "                    pointer[i][j] = k\n",
    "    # decoding: 把最好的tag seq打印出来\n",
    "    best_seq = [0 for _ in range(T)]\n",
    "    # step1： 找出对应于最后一个单词的词性\n",
    "    best_seq[T-1] = np.argmax(dp[T-1])\n",
    "    # step2: 通过从后到前的循环依次求出每个单词的词性\n",
    "    for i in range(T-2, -1, -1):\n",
    "        best_seq[i] = pointer[i+1][best_seq[i+1]]\n",
    "    # 到目前为止 best_seq存放了对应于x的词性序列\n",
    "    for i in range(len(best_seq)):\n",
    "        print(id2tag[best_seq[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Oscar\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.784 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "云南 完成 党报 党刊 发行 任务\n",
      "ns\n",
      "v\n",
      "n\n",
      "n\n",
      "vn\n",
      "n\n"
     ]
    }
   ],
   "source": [
    "sentence = '云南完成党报党刊发行任务'\n",
    "print(' '.join(jieba.cut(sentence)))\n",
    "viterbi(sentence, pi, A, B)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
